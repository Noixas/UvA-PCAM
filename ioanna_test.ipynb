{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:54:00.446276172Z",
     "start_time": "2023-09-21T08:53:57.769218951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(Deprecated) Installing extensions with the jupyter labextension install command is now deprecated and will be removed in a future major version of JupyterLab.\r\n",
      "\r\n",
      "Users should manage prebuilt extensions with package managers like pip and conda, and extension authors are encouraged to distribute their extensions as prebuilt packages \u001B[0m\r\n",
      "/home/ioanna/miniconda3/envs/medical/lib/python3.8/site-packages/jupyterlab/debuglog.py:56: UserWarning: An error occurred.\r\n",
      "  warnings.warn(\"An error occurred.\")\r\n",
      "/home/ioanna/miniconda3/envs/medical/lib/python3.8/site-packages/jupyterlab/debuglog.py:57: UserWarning: ValueError: Please install nodejs >=18.0.0 before continuing. nodejs may be installed using conda or directly from the nodejs website.\r\n",
      "  warnings.warn(msg[-1].strip())\r\n",
      "/home/ioanna/miniconda3/envs/medical/lib/python3.8/site-packages/jupyterlab/debuglog.py:58: UserWarning: See the log file for details: /tmp/jupyterlab-debug-07nutyjz.log\r\n",
      "  warnings.warn(f\"See the log file for details: {log_path!s}\")\r\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\r\n",
      "               [--paths] [--json] [--debug]\r\n",
      "               [subcommand]\r\n",
      "\r\n",
      "Jupyter: Interactive Computing\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  subcommand     the subcommand to launch\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help     show this help message and exit\r\n",
      "  --version      show the versions of core jupyter packages and exit\r\n",
      "  --config-dir   show Jupyter config dir\r\n",
      "  --data-dir     show Jupyter data dir\r\n",
      "  --runtime-dir  show Jupyter runtime dir\r\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\r\n",
      "                 format.\r\n",
      "  --json         output paths as machine-readable json\r\n",
      "  --debug        output debug information about paths\r\n",
      "\r\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\r\n",
      "labextension labhub migrate nbconvert notebook qtconsole run server\r\n",
      "troubleshoot trust\r\n",
      "\r\n",
      "Jupyter command `jupyter-nbextension` not found.\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.models import alexnet, vgg16, googlenet, inception_v3, resnet18, densenet161\n",
    "from torchvision.datasets import PCAM\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torcheval.metrics import MulticlassAUROC, MulticlassAccuracy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as _tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:54:04.456277091Z",
     "start_time": "2023-09-21T08:54:04.454913208Z"
    }
   },
   "id": "f6b0e93bcef244ef"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Dataset and data loaders\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = PCAM(root='data', split='train', download=True, transform=transform)\n",
    "val_dataset = PCAM(root='data', split='val', download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:54:04.884204222Z",
     "start_time": "2023-09-21T08:54:04.847296529Z"
    }
   },
   "id": "ec8883ce1697a405"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Select(description='Model:', options=(('AlexNet', <function alexnet at 0x7fafaefcaa60>), ('VGG-16', <function …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd372c4484a54fd390cd2b8902ef39c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model Selection\n",
    "\n",
    "model_widget = widgets.Select(\n",
    "    options=[('AlexNet', alexnet), ('VGG-16', vgg16), ('GoogleNet', googlenet), ('Inception-v3', inception_v3), ('ResNet-18', resnet18), ('DenseNet-161', densenet161)],\n",
    "    value=alexnet,\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(model_widget)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T09:32:05.937031905Z",
     "start_time": "2023-09-21T09:32:05.888540479Z"
    }
   },
   "id": "ab71cba5a3dbe89a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: resnet18\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected Model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_widget\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m model_widget\u001B[38;5;241m.\u001B[39mvalue(pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Freeze all layers except last\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mparameters():\n",
      "File \u001B[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "## Model Initialization\n",
    " \n",
    "print(f'Selected Model: {model_widget.value.__name__}')\n",
    "model = model_widget.value(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except last\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create classification layer    \n",
    "num_classes = 2\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "## Optimizer\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "## Loss Function\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T09:33:32.239926627Z",
     "start_time": "2023-09-21T09:33:31.996236094Z"
    }
   },
   "id": "91fdafbb67b1c3d2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def uniquify(path):\n",
    "    \"\"\"\n",
    "    Creates unique path name by appending number if given path already exists\n",
    "    \"\"\"\n",
    "    \n",
    "    filename, extension = os.path.splitext(path)\n",
    "    counter = 1\n",
    "\n",
    "    while os.path.exists(path):\n",
    "        path = filename + \"_\" + str(counter) + extension\n",
    "        counter += 1\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for loop progress bar\n",
    "    \"\"\"\n",
    "    \n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, loss_fun, optimizer, num_epochs, num_classes, device, save_ckpt_path=None):\n",
    "    \"\"\"\n",
    "    Trains model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Create metric monitors\n",
    "    auc = MulticlassAUROC(num_classes=num_classes)\n",
    "    accuracy = MulticlassAccuracy()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize the running loss and metrics\n",
    "        curr_loss = 0.0\n",
    "        auc.reset()\n",
    "        accuracy.reset()\n",
    "        \n",
    "        ## Train\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}, Training'):\n",
    "            \n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_fun(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss and metrics\n",
    "            curr_loss += loss.item() * inputs.size(0)\n",
    "            auc.update(outputs, labels)\n",
    "            accuracy.update(outputs, labels)\n",
    "\n",
    "        # Calculate the train loss and metrics\n",
    "        train_loss = curr_loss / len(train_dataset)\n",
    "        train_acc = accuracy.compute()\n",
    "        train_auc = auc.compute()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize the running loss\n",
    "        curr_loss = 0.0\n",
    "        \n",
    "        # Initialize the metrics\n",
    "        auc.reset()\n",
    "        accuracy.reset()\n",
    "\n",
    "        ## Validate\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs}, Validation'):\n",
    "                \n",
    "                # Move the inputs and labels to the device\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = loss_fun(outputs, labels)\n",
    "\n",
    "                # Update the running loss and metrics\n",
    "                curr_loss += loss.item() * inputs.size(0)\n",
    "                auc.update(outputs, labels)\n",
    "                accuracy.update(outputs, labels)\n",
    "\n",
    "        # Calculate the validation loss, accuracy and AUC\n",
    "        val_loss = curr_loss / len(val_dataset)\n",
    "        val_acc = accuracy.compute()\n",
    "        val_auc = auc.compute()\n",
    "\n",
    "        # Print the epoch results\n",
    "        print('Train Loss: {:.4f}, Train Acc: {:.4f}, Train AUC: {:.4f}, \\n Val Loss: {:.4f}, Val Acc: {:.4f}, Val AUC: {:.4f}\\n'\n",
    "              .format(train_loss, train_acc, train_auc, val_loss, val_acc, val_auc))\n",
    "        \n",
    "        ## Save model checkpoint\n",
    "        if save_ckpt_path is None:\n",
    "            save_ckpt_path = os.path.join('models',f'{model.__class__.__name__}.pt')\n",
    "            if not os.path.exists('models'):  # If folder 'models' doesn't exist, create it\n",
    "                os.makedirs('models')\n",
    "        save_ckpt_path = uniquify(save_ckpt_path)  # Create unique path name by appending number if given path already exists\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_auc': train_auc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc':val_acc,\n",
    "            'val_auc': val_auc,\n",
    "            }, save_ckpt_path)\n",
    "        print(f'Saved checkpoint at: {save_ckpt_path}')\n",
    "        \n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T19:37:14.480042337Z",
     "start_time": "2023-09-20T19:37:14.479124872Z"
    }
   },
   "id": "ec3108b8876c27ac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training: 100%|██████████| 8192/8192 [04:18<00:00, 31.64it/s]\n",
      "Epoch 1/5, Validation: 100%|██████████| 1024/1024 [00:29<00:00, 34.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9527, Train Acc: 0.7503, Train AUC: 0.8198, \n",
      " Val Loss: 1.0491, Val Acc: 0.7138, Val AUC: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training: 100%|██████████| 8192/8192 [04:00<00:00, 34.12it/s]\n",
      "Epoch 2/5, Validation: 100%|██████████| 1024/1024 [00:28<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9426, Train Acc: 0.7517, Train AUC: 0.8207, \n",
      " Val Loss: 1.5198, Val Acc: 0.6844, Val AUC: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training: 100%|██████████| 8192/8192 [03:57<00:00, 34.42it/s]\n",
      "Epoch 3/5, Validation: 100%|██████████| 1024/1024 [00:31<00:00, 32.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9563, Train Acc: 0.7496, Train AUC: 0.8196, \n",
      " Val Loss: 1.2499, Val Acc: 0.7279, Val AUC: 0.8508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training: 100%|██████████| 8192/8192 [04:19<00:00, 31.59it/s]\n",
      "Epoch 4/5, Validation: 100%|██████████| 1024/1024 [00:32<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9953, Train Acc: 0.7487, Train AUC: 0.8180, \n",
      " Val Loss: 0.7975, Val Acc: 0.7909, Val AUC: 0.8697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training: 100%|██████████| 8192/8192 [04:24<00:00, 30.97it/s]\n",
      "Epoch 5/5, Validation: 100%|██████████| 1024/1024 [00:36<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9534, Train Acc: 0.7523, Train AUC: 0.8222, \n",
      " Val Loss: 0.8085, Val Acc: 0.7602, Val AUC: 0.8360\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, loss_fun, optimizer, num_epochs=5, num_classes = 2, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T20:00:54.848320462Z",
     "start_time": "2023-09-20T19:37:14.482159016Z"
    }
   },
   "id": "9a5fc95dcb80dedc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a92f49a66ac98aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
